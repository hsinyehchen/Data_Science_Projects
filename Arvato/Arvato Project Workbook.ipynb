{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-318b66be2894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load in the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mazdias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Udacity_AZDIAS_052018.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcustomers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Udacity_CUSTOMERS_052018.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/core/dtypes/common.pyc\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m     \"\"\"\n\u001b[1;32m    813\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('./Udacity_AZDIAS_052018.csv', sep=';')\n",
    "customers = pd.read_csv('./Udacity_CUSTOMERS_052018.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "#### 1. Data Cleaning - convert unknown values to `NaN`\n",
    "The first step is to look at the features and the range of possible of each range. The description of each column can be found in `DIAS Information Levels - Attributes 2017.xlsx` and `DIAS Attributes - Values 2017.xlsx`. The values of each feature contains different ranges, and some represents \"unknown\". The first step is to manually convert these \"unknowns\" to NaN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) read features from attribute file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read attribute file from xlsx\n",
    "ATTRIBUTE_FILE=\"DIAS Attributes - Values 2017.xlsx\"\n",
    "df_Levels_Attributes = pd.read_excel(ATTRIBUTE_FILE)\n",
    "\n",
    "# This step can fill the attribute names in the consecutive rows.\n",
    "for i in range(len(df_Levels_Attributes[\"Attribute\"])):\n",
    "    if pd.isna(df_Levels_Attributes.iloc[i][\"Attribute\"]):\n",
    "        df_Levels_Attributes.at[i,\"Attribute\"] = attr_name\n",
    "    else:\n",
    "        attr_name = df_Levels_Attributes.iloc[i][\"Attribute\"]\n",
    "\n",
    "# Parse the attribute file to get values representing \"unknown\"\n",
    "COLUMN_UNKNOWN_VALUE = dict()\n",
    "for index, row in df_Levels_Attributes[df_Levels_Attributes[\"Meaning\"] == \"unknown\"].iterrows():\n",
    "    COLUMN_UNKNOWN_VALUE[row[\"Attribute\"]] = [int(n) for n in str(row[\"Value\"]).split(\",\")]\n",
    "\n",
    "# These are the columns for integer. So the value would be \"...\"\n",
    "INTEGER_LABELS_COLUMN = {\"ANZ_HAUSHALTE_AKTIV\", \"ANZ_HH_TITEL\", \"ANZ_PERSONEN\", \n",
    "                         \"ANZ_TITEL\", \"GEBURTSJAHR\", \"KBA13_ANZAHL_PKW\", \"MIN_GEBAEUDEJAHR\"}\n",
    "\n",
    "# Construct the unknown and acceptable values for each column.\n",
    "COLUMN_LABELS = dict()\n",
    "for index, row in df_Levels_Attributes[df_Levels_Attributes[\"Meaning\"] != \"unknown\"].iterrows():\n",
    "    if row[\"Attribute\"] not in COLUMN_LABELS:\n",
    "        COLUMN_LABELS[row[\"Attribute\"]] = []\n",
    "    if row[\"Attribute\"] in INTEGER_LABELS_COLUMN:\n",
    "        continue\n",
    "    COLUMN_LABELS[row[\"Attribute\"]] += str(row[\"Value\"]).split(\",\")\n",
    "\n",
    "# Check the difference columns in two datasets\n",
    "print(set(customers.columns) - set(azdias.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Drop undocumented features\n",
    "Many features in the azdias and customers dataframe are not explained in the attribute file. These columns are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the missing undocumented features\n",
    "UNDOCUMENTED_COLUMN = set(azdias.columns) - set(COLUMN_LABELS.keys())\n",
    "print(UNDOCUMENTED_COLUMN)\n",
    "\n",
    "# LNR should be sequence number of samples. Dropped from data frames\n",
    "azdias = azdias.drop(columns={\"LNR\"})\n",
    "customers = customers.drop(columns=\"LNR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Convert values that represents \"unknown\" to `NaN`\n",
    "In some columns, the value of `0` or `-1` actually means unknown,or some values are out-of-range. These data are replaced with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the unknown values with NaN\n",
    "def fill_unknown_values(df):\n",
    "    MISSING_COLUMN = set(COLUMN_LABELS.keys()) - set(df.columns)\n",
    "    for column_name in COLUMN_LABELS:\n",
    "        # Skip undocumneted columns\n",
    "        if column_name in MISSING_COLUMN:\n",
    "            continue\n",
    "            \n",
    "        # Skip integer columns\n",
    "        if column_name in INTEGER_LABELS_COLUMN:\n",
    "            continue\n",
    "        \n",
    "        # Find the out-of-range value and unknown values\n",
    "        nan_mask = ~df[column_name].isin(COLUMN_LABELS[column_name])\n",
    "        if column_name in COLUMN_UNKNOWN_VALUE:\n",
    "            nan_mask |= df[column_name].isin(COLUMN_UNKNOWN_VALUE[column_name])\n",
    "        # Fill nan\n",
    "        df.loc[nan_mask, column_name] = np.nan\n",
    "\n",
    "    # special handling\n",
    "    df.loc[df[\"GEBURTSJAHR\"] == 0, \"GEBURTSJAHR\"] = np.nan\n",
    "\n",
    "\n",
    "fill_unknown_values(azdias)\n",
    "fill_unknown_values(customers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d) Handle numerical strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numerica strings to int\n",
    "def convert_str_to_int(df, column):\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "convert_str_to_int(azdias, \"CAMEO_DEUG_2015\")\n",
    "convert_str_to_int(azdias, \"CAMEO_INTL_2015\")\n",
    "convert_str_to_int(customers, \"CAMEO_DEUG_2015\")\n",
    "convert_str_to_int(customers, \"CAMEO_INTL_2015\")\n",
    "\n",
    "# convert the time stamp to number of days after 1900-01-01\n",
    "azdias['EINGEFUEGT_AM'] = pd.to_datetime(azdias['EINGEFUEGT_AM']).sub(pd.Timestamp('1900-01-01')).dt.days\n",
    "customers['EINGEFUEGT_AM'] = pd.to_datetime(customers['EINGEFUEGT_AM']).sub(pd.Timestamp('1900-01-01')).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e) Check the percetage of NaN values in each column\n",
    "The majority of NaN is less than 0.15% and at most 0.40% of total features in both of dataset, so it would not have significant impact on the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summation over all the nan samples in each column\n",
    "azdias_nan_counter = {column_name: np.sum(azdias[column_name].isna()) for column_name in azdias.columns}\n",
    "customers_nan_counter = {column_name: np.sum(customers[column_name].isna()) for column_name in customers.columns}\n",
    "\n",
    "# Compute the percentage\n",
    "azdias_na_percent = [100.0*v/float(azdias.size) for v in azdias_nan_counter.values()]\n",
    "customers_na_percent = [100.0*v/float(customers.size) for v in customers_nan_counter.values()]\n",
    "\n",
    "plt.hist([azdias_na_percent, customers_na_percent], bins = 25, label=[\"azdias nan\", \"customers nan\"])\n",
    "\n",
    "# Plot formatting\n",
    "plt.legend()\n",
    "plt.xlabel('Percentage of NaN in column (%)')\n",
    "plt.ylabel('Number of columns')\n",
    "plt.title('Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning - Handling catagorized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data of two categories to 0, 1\n",
    "def convert_biclass_to_int(df, column):\n",
    "    nan_mask = df[column].notna()\n",
    "    df.loc[nan_mask, column] = df[column][nan_mask].astype('category').cat.codes\n",
    "\n",
    "convert_biclass_to_int(azdias, \"OST_WEST_KZ\")\n",
    "convert_biclass_to_int(customers, \"OST_WEST_KZ\")\n",
    "print(azdias.groupby(\"OST_WEST_KZ\").groups.keys())\n",
    "print(customers.groupby(\"OST_WEST_KZ\").groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data of two categories to one-hot-encoding\n",
    "def convert_categories_to_onehot(df, column):\n",
    "    nan_mask = df[column].notna()\n",
    "    df.loc[nan_mask, column] = df[column][nan_mask].astype('category')\n",
    "    \n",
    "    # Get one-hot encoding\n",
    "    onehot = pd.get_dummies(df[column], prefix=column, dummy_na=True)\n",
    "    df = pd.concat([df, onehot], axis=1)\n",
    "    \n",
    "    # drop the original column\n",
    "    df = df.drop(columns=[column])\n",
    "\n",
    "    return df\n",
    "\n",
    "azdias = convert_categories_to_onehot(azdias, \"CAMEO_DEU_2015\")\n",
    "customers = convert_categories_to_onehot(customers, \"CAMEO_DEU_2015\")\n",
    "azdias = convert_categories_to_onehot(azdias, \"D19_LETZTER_KAUF_BRANCHE\")\n",
    "customers = convert_categories_to_onehot(customers, \"D19_LETZTER_KAUF_BRANCHE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print groups in each column to double check all the features are within range.\n",
    "def print_groups(df):\n",
    "    for column_name in df.columns:\n",
    "        keys = df.groupby(column_name).groups.keys()\n",
    "\n",
    "        # Skip the columns which has numbers from 0~24 to reduce the printing\n",
    "        if set(keys) <= set(range(25)):\n",
    "            continue\n",
    "        print(\"***** {} *****\".format(column_name))\n",
    "        print(keys)\n",
    "\n",
    "\n",
    "print_groups(azdias)\n",
    "print_groups(customers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning - Use mean instead of `Nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(customers.columns)\n",
    "columns.remove(\"CUSTOMER_GROUP\")\n",
    "columns.remove(\"ONLINE_PURCHASE\")\n",
    "columns.remove(\"PRODUCT_GROUP\")\n",
    "\n",
    "# Replace NaN with means \n",
    "for c in columns:\n",
    "    customers[c].fillna(customers[c].mean(), inplace=True)\n",
    "    azdias[c].fillna(azdias[c].mean(), inplace=True)\n",
    "\n",
    "# Double check if all the NaN are removed.\n",
    "print([col for col in customers.columns if np.sum(customers[col].isna()) > 0])\n",
    "print([col for col in azdias.columns if np.sum(azdias[col].isna()) > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization with mean-std. This was the original step, but didn't work well.\n",
    "#mean_azdias = azdias.mean()\n",
    "#std_azdias = azdias.std()\n",
    "#normalized_azdias = (azdias - mean_azdias)/std_azdias\n",
    "\n",
    "# Normalization with min-max\n",
    "max_azdias = azdias.max()\n",
    "min_azdias = azdias.min()\n",
    "normalized_azdias = (azdias - min_azdias)/(max_azdias - min_azdias)\n",
    "normalized_azdias.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principle Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "azdias_rotated = pca.fit_transform(normalized_azdias)\n",
    "explained_variance_by_k = pca.explained_variance_ratio_.cumsum()\n",
    "plt.plot(range(1,len(explained_variance_by_k)+1),explained_variance_by_k,marker=\"*\")\n",
    "plt.xlabel(\"Num. of Components\")\n",
    "plt.ylabel(\"Cumulative Power Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 150 componants accommodate 90% of energy. However, there is no significant transition in the energy. So we can use the entire feature sets for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The elbow is around 260 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# choose best K (i.e., number of clusters)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "print(normalized_azdias.shape)\n",
    "\n",
    "inertias_mini = []\n",
    "\n",
    "ks = range(10,601,10)\n",
    "for k in ks:\n",
    "    # execute the K-Means on the range. Append results to inertias_mini\n",
    "    print(\"**** {} ****\".format(k))\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, verbose=0)\n",
    "    kmeans.fit(normalized_azdias.values)\n",
    "    inertias_mini.append(kmeans.inertia_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.plot(ks,inertias_mini,marker='x')# want to use elbow method to find best k\n",
    "\n",
    "plt.xlabel(\"Num. of Cluster\")\n",
    "plt.ylabel(\"Inertia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The elbow is around 240, the kmeans is re-trained and predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=240, verbose=1)\n",
    "kmeans.fit(normalized_azdias.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize customers with the min-max from AZDIAS.\n",
    "normalized_customers = (customers[azdias.columns] - min_azdias)/(max_azdias - min_azdias)\n",
    "\n",
    "# Double check NaN values\n",
    "print([k for k in normalized_customers.columns if np.sum(normalized_customers[k].isna()) > 0])\n",
    "\n",
    "# Make clustering predictions\n",
    "pred = kmeans.predict(normalized_customers[list(azdias.columns)].values)\n",
    "\n",
    "# Create data frame\n",
    "pred = pd.DataFrame({'cluster': pred})\n",
    "print(pred)\n",
    "cluster_group = pred.groupby('cluster').groups\n",
    "print(cluster_group.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the clustering results, the top 1 cluster is cluster-51, which accounts for 12.25% of entire customer base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Sort the cluster based on the number of samples in the cluster\n",
    "sorted_cluster = sorted([(len(cluster_group[k]), k, len(cluster_group[k])/float(pred.shape[0])) for k in cluster_group], reverse=True)\n",
    "#print(sorted_cluster[:10])\n",
    "#print(\"\\n\".join([\"{}\\t{}\\t{:.3f}\".format(v[1],v[0],100.0*v[2]) for v in sorted_cluster[:10]]))\n",
    "#print(sum([v[2] for v in sorted_cluster[:5]]))\n",
    "\n",
    "# The top-1 cluster is the following ID\n",
    "centroid_id = sorted_cluster[0][1]\n",
    "print(centroid_id)\n",
    "\n",
    "\n",
    "# Display the component of each cluster center\n",
    "def display_component(centroid_id, max_azdias, min_azdias, n_weights = 10):\n",
    "    \n",
    "    centroid = kmeans.cluster_centers_[centroid_id]\n",
    "    \n",
    "    # Create data frame of centroid features vs columns names\n",
    "    comp = pd.DataFrame(list(zip(centroid, list(azdias.columns))), columns=['scaled_weights', 'features'])\n",
    "    \n",
    "    # Append into data frame, the absolute weights of each centroid feature \n",
    "    comp['abs_weights'] = comp['scaled_weights'].apply(lambda x: np.abs(x))\n",
    "\n",
    "    # Append into data frame, the original weights of each centroid feature \n",
    "    comp['original_weights'] = comp.apply(lambda row: (row.scaled_weights*(max_azdias[row.features] - min_azdias[row.features]) + min_azdias[row.features]), axis = 1) \n",
    "\n",
    "    # Sort and copy the first 10 features\n",
    "    sorted_weight_data = comp.sort_values('abs_weights', ascending=False).head(n_weights)\n",
    "    \n",
    "    # Output figures\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # output weights after normalization\n",
    "    ax=fig.add_subplot(2,1,1)\n",
    "    ax=sns.barplot(data=sorted_weight_data, \n",
    "                   x=\"scaled_weights\", \n",
    "                   y=\"features\", \n",
    "                   palette=\"Blues_d\")\n",
    "    ax.set_title(\"Cluster centroid\")\n",
    "\n",
    "    # output weights before normalization (original)\n",
    "    ax=fig.add_subplot(2,1,2)\n",
    "    ax=sns.barplot(data=sorted_weight_data, \n",
    "                   x=\"original_weights\", \n",
    "                   y=\"features\", \n",
    "                   palette=\"Blues_d\")\n",
    "    ax.set_title(\"Cluster centroid\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cluster_{}.png\".format(centroid_id))\n",
    "    plt.show()\n",
    "\n",
    "# Output the top-1 centroid\n",
    "display_component(centroid_id, max_azdias, min_azdias)\n",
    "\n",
    "# Output the 2nd populous centroid\n",
    "display_component(sorted_cluster[1][1], max_azdias, min_azdias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the union set of 10 most significant weights in the top-5 populous clusters\n",
    "feature_set = set()\n",
    "for num, cen_id, percentage in sorted_cluster[:5]:\n",
    "    centroid = kmeans.cluster_centers_[cen_id]\n",
    "    comp = pd.DataFrame(list(zip(centroid, list(azdias.columns))), columns=['scaled_weights', 'features'])\n",
    "    comp['abs_weights'] = comp['scaled_weights'].apply(lambda x: np.abs(x))\n",
    "    sorted_weight_data = comp.sort_values('abs_weights', ascending=False).head(10)\n",
    "    feature_set = feature_set | set(sorted_weight_data[\"features\"])\n",
    "\n",
    "# Convert set to list\n",
    "feature_list = list(feature_set)\n",
    "\n",
    "# Print out the feature list\n",
    "print(\"The most significant weights in the top-5 populous clusters are:\")\n",
    "print(\"\\n\".join(feature_list))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print out the corresponding weights in the features of each cluster centroids\n",
    "for i in range(5):\n",
    "    num, cen_id, percentage = sorted_cluster[i]\n",
    "\n",
    "    centroid = kmeans.cluster_centers_[cen_id]\n",
    "    comp = pd.DataFrame(list(zip(centroid, list(azdias.columns))), columns=['scaled_weights', 'features'])\n",
    "    comp['abs_weights'] = comp['scaled_weights'].apply(lambda x: np.abs(x))\n",
    "    comp['original_weights'] = comp.apply(lambda row: (row.scaled_weights*(max_azdias[row.features] - min_azdias[row.features]) + min_azdias[row.features]), axis = 1) \n",
    "\n",
    "    # Convert the weights to string with tab separated.\n",
    "    ss = [\"Cluster - \" + str(cen_id), str(num), \"{:.2f}\".format(100*percentage)]\n",
    "    for j, fr in enumerate(feature_list):\n",
    "        orig_w = comp[comp['features']==fr]['original_weights']\n",
    "        orig_w = np.int(np.round(orig_w))\n",
    "        ss.append(str(orig_w))\n",
    "    print(\"\\t\".join(ss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('Udacity_MAILOUT_052018_TRAIN.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data cleaning - handling `NaN`, time stamps, categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill uknown and out-of-range values with NaN\n",
    "fill_unknown_values(mailout_train)\n",
    "\n",
    "# Convert strings to integers\n",
    "convert_str_to_int(mailout_train, \"CAMEO_DEUG_2015\")\n",
    "convert_str_to_int(mailout_train, \"CAMEO_INTL_2015\")\n",
    "\n",
    "# Convert 2-class data to 0,1\n",
    "convert_biclass_to_int(mailout_train, \"OST_WEST_KZ\")\n",
    "\n",
    "# Convert categorical data to one-hot encoding\n",
    "try:\n",
    "    mailout_train = convert_categories_to_onehot(mailout_train, \"CAMEO_DEU_2015\")\n",
    "    mailout_train = convert_categories_to_onehot(mailout_train, \"D19_LETZTER_KAUF_BRANCHE\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Convert time stamps to days after 1900-01-01\n",
    "mailout_train['EINGEFUEGT_AM'] = pd.to_datetime(mailout_train['EINGEFUEGT_AM']).sub(pd.Timestamp('1900-01-01')).dt.days\n",
    "\n",
    "# Double check the values in each column\n",
    "for column_name in mailout_train.columns:\n",
    "    \n",
    "    keys = mailout_train.groupby(column_name).groups.keys() \n",
    "    \n",
    "    # Skip the columns with intergers of 0~49\n",
    "    if set(keys) <= set(range(50)):\n",
    "        continue\n",
    "\n",
    "    print(column_name)\n",
    "    print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NaN with means\n",
    "for c in mailout_train.columns:\n",
    "    mailout_train[c].fillna(mailout_train[c].mean(), inplace=True)\n",
    "\n",
    "# Double check if there is still NaN value. The output should empty list.\n",
    "print([col for col in mailout_train.columns if np.sum(mailout_train[col].isna()) > 0])\n",
    "\n",
    "# Save data frame to local file\n",
    "mailout_train.to_pickle(\"mailout_train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalization with min-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data frame from local file\n",
    "mailout_train = pd.read_pickle(\"mailout_train.pkl\")\n",
    "\n",
    "# Apply normalization\n",
    "max_mailout_train = mailout_train.max()\n",
    "min_mailout_train = mailout_train.min()\n",
    "normalized_mailout_train = (mailout_train - min_mailout_train)/(max_mailout_train - min_mailout_train)\n",
    "normalized_mailout_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Principle Component Analysis\n",
    "The PCA shows that the 60% of features hold 90% of energy, but no significat cutting points in dimension. Here the dimensions are kept for easier interpretation of decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mailout_rotated = pca.fit_transform(normalized_mailout_train)\n",
    "explained_variance_by_k = pca.explained_variance_ratio_.cumsum()\n",
    "plt.plot(range(1,len(explained_variance_by_k)+1),explained_variance_by_k,marker=\"*\")\n",
    "plt.xlabel(\"Num. of Components\")\n",
    "plt.ylabel(\"Cumulative Power Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Supervise Learning - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training data into XGBoost data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Need to add this env variable so the XGBoost can be executed.\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from sklearn.cross_validation import  train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "seed = 999\n",
    "# Separate input (Features) and output (REPONSE)\n",
    "X = normalized_mailout_train.loc[:,normalized_mailout_train.columns != 'RESPONSE']\n",
    "y = normalized_mailout_train[\"RESPONSE\"]\n",
    "\n",
    "# Split into training dataset and test dataset\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "train_matrix = xgb.DMatrix(Xtrain, ytrain)\n",
    "test_matrix = xgb.DMatrix(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for XGBoost.\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'  # output probabilities\n",
    "params['eval_metric'] = 'auc'\n",
    "params[\"num_rounds\"] = 300\n",
    "params[\"early_stopping_rounds\"] = 30\n",
    "# params['min_child_weight'] = 2\n",
    "params['max_depth'] = 6\n",
    "params['eta'] = 0.1\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.8\n",
    "\n",
    "# Cross validation of XGBoost to find the best iteration.\n",
    "cv_results = xgb.cv(params, train_matrix,\n",
    "                    num_boost_round = params[\"num_rounds\"],\n",
    "                    nfold = params.get('nfold',5),\n",
    "                    metrics = params['eval_metric'],\n",
    "                    early_stopping_rounds = params[\"early_stopping_rounds\"],\n",
    "                    verbose_eval = True,\n",
    "                    seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out cross validation results\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best iteration of decision trees\n",
    "n_best_trees = cv_results.shape[0]\n",
    "n_best_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the XGBoost on the entire training set\n",
    "watchlist = [(train_matrix, 'train')]\n",
    "gbt = xgb.train(params, train_matrix, n_best_trees,watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output the ROC curve and calculate the ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plotting function\n",
    "def plot_validation_roc():\n",
    "    Xtrain_only,Xvalid,ytrain_only,yvalid = train_test_split(Xtrain,ytrain,test_size=0.3,random_state=seed)\n",
    "    print(Xvalid.shape)\n",
    "    valid_matrix = xgb.DMatrix(Xvalid)\n",
    "\n",
    "    temp_gbt = gbt\n",
    "    yvalid_proba_pred = gbt.predict(valid_matrix,ntree_limit=n_best_trees)\n",
    "    print(len(yvalid_proba_pred))\n",
    "    fpr,tpr,thresholds = roc_curve(yvalid,yvalid_proba_pred)\n",
    "    print(\"auc = {}\".format(auc(fpr,tpr)))\n",
    "    return pd.DataFrame({'FPR':fpr,'TPR':tpr,'Threshold':thresholds})\n",
    "\n",
    "roc = plot_validation_roc()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(roc.FPR,roc.TPR,marker='h')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer â€“ this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test = pd.read_csv('Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data cleaning - handle `NaN`, time stamp, and categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNR</th>\n",
       "      <th>AGER_TYP</th>\n",
       "      <th>AKT_DAT_KL</th>\n",
       "      <th>ALTER_HH</th>\n",
       "      <th>ALTER_KIND1</th>\n",
       "      <th>ALTER_KIND2</th>\n",
       "      <th>ALTER_KIND3</th>\n",
       "      <th>ALTER_KIND4</th>\n",
       "      <th>ALTERSKATEGORIE_FEIN</th>\n",
       "      <th>ANZ_HAUSHALTE_AKTIV</th>\n",
       "      <th>...</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_TECHNIK</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_TELKO_MOBILE</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_TELKO_REST</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_TIERARTIKEL</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_UNBEKANNT</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_VERSAND_REST</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_VERSICHERUNGEN</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_VOLLSORTIMENT</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_D19_WEIN_FEINKOST</th>\n",
       "      <th>D19_LETZTER_KAUF_BRANCHE_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.00000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "      <td>42833.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.501109</td>\n",
       "      <td>0.550505</td>\n",
       "      <td>0.064861</td>\n",
       "      <td>0.487596</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.710161</td>\n",
       "      <td>0.703566</td>\n",
       "      <td>0.601140</td>\n",
       "      <td>0.392903</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009409</td>\n",
       "      <td>0.012280</td>\n",
       "      <td>0.012467</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.23615</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.060141</td>\n",
       "      <td>0.054047</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>0.175706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288554</td>\n",
       "      <td>0.179112</td>\n",
       "      <td>0.198950</td>\n",
       "      <td>0.266515</td>\n",
       "      <td>0.054131</td>\n",
       "      <td>0.029916</td>\n",
       "      <td>0.015871</td>\n",
       "      <td>0.007544</td>\n",
       "      <td>0.158840</td>\n",
       "      <td>0.035498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096542</td>\n",
       "      <td>0.110135</td>\n",
       "      <td>0.110959</td>\n",
       "      <td>0.052856</td>\n",
       "      <td>0.42472</td>\n",
       "      <td>0.142014</td>\n",
       "      <td>0.237750</td>\n",
       "      <td>0.226113</td>\n",
       "      <td>0.082425</td>\n",
       "      <td>0.380574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.252331</td>\n",
       "      <td>0.550505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.710161</td>\n",
       "      <td>0.703566</td>\n",
       "      <td>0.601140</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.501818</td>\n",
       "      <td>0.550505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.487596</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.710161</td>\n",
       "      <td>0.703566</td>\n",
       "      <td>0.601140</td>\n",
       "      <td>0.392903</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750070</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.064861</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.710161</td>\n",
       "      <td>0.703566</td>\n",
       "      <td>0.601140</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 445 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                LNR      AGER_TYP    AKT_DAT_KL      ALTER_HH   ALTER_KIND1  \\\n",
       "count  42833.000000  42833.000000  42833.000000  42833.000000  42833.000000   \n",
       "mean       0.501109      0.550505      0.064861      0.487596      0.658377   \n",
       "std        0.288554      0.179112      0.198950      0.266515      0.054131   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.252331      0.550505      0.000000      0.380952      0.658377   \n",
       "50%        0.501818      0.550505      0.000000      0.487596      0.658377   \n",
       "75%        0.750070      0.666667      0.064861      0.619048      0.658377   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "        ALTER_KIND2   ALTER_KIND3   ALTER_KIND4  ALTERSKATEGORIE_FEIN  \\\n",
       "count  42833.000000  42833.000000  42833.000000          42833.000000   \n",
       "mean       0.710161      0.703566      0.601140              0.392903   \n",
       "std        0.029916      0.015871      0.007544              0.158840   \n",
       "min        0.000000      0.000000      0.000000              0.000000   \n",
       "25%        0.710161      0.703566      0.601140              0.360000   \n",
       "50%        0.710161      0.703566      0.601140              0.392903   \n",
       "75%        0.710161      0.703566      0.601140              0.480000   \n",
       "max        1.000000      1.000000      1.000000              1.000000   \n",
       "\n",
       "       ANZ_HAUSHALTE_AKTIV              ...               \\\n",
       "count         42833.000000              ...                \n",
       "mean              0.017810              ...                \n",
       "std               0.035498              ...                \n",
       "min               0.000000              ...                \n",
       "25%               0.002639              ...                \n",
       "50%               0.007916              ...                \n",
       "75%               0.017810              ...                \n",
       "max               1.000000              ...                \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_D19_TECHNIK  \\\n",
       "count                          42833.000000   \n",
       "mean                               0.009409   \n",
       "std                                0.096542   \n",
       "min                                0.000000   \n",
       "25%                                0.000000   \n",
       "50%                                0.000000   \n",
       "75%                                0.000000   \n",
       "max                                1.000000   \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_D19_TELKO_MOBILE  \\\n",
       "count                               42833.000000   \n",
       "mean                                    0.012280   \n",
       "std                                     0.110135   \n",
       "min                                     0.000000   \n",
       "25%                                     0.000000   \n",
       "50%                                     0.000000   \n",
       "75%                                     0.000000   \n",
       "max                                     1.000000   \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_D19_TELKO_REST  \\\n",
       "count                             42833.000000   \n",
       "mean                                  0.012467   \n",
       "std                                   0.110959   \n",
       "min                                   0.000000   \n",
       "25%                                   0.000000   \n",
       "50%                                   0.000000   \n",
       "75%                                   0.000000   \n",
       "max                                   1.000000   \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_D19_TIERARTIKEL  \\\n",
       "count                              42833.000000   \n",
       "mean                                   0.002802   \n",
       "std                                    0.052856   \n",
       "min                                    0.000000   \n",
       "25%                                    0.000000   \n",
       "50%                                    0.000000   \n",
       "75%                                    0.000000   \n",
       "max                                    1.000000   \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_D19_UNBEKANNT  \\\n",
       "count                             42833.00000   \n",
       "mean                                  0.23615   \n",
       "std                                   0.42472   \n",
       "min                                   0.00000   \n",
       "25%                                   0.00000   \n",
       "50%                                   0.00000   \n",
       "75%                                   0.00000   \n",
       "max                                   1.00000   \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_D19_VERSAND_REST  \\\n",
       "count                               42833.000000   \n",
       "mean                                    0.020592   \n",
       "std                                     0.142014   \n",
       "min                                     0.000000   \n",
       "25%                                     0.000000   \n",
       "50%                                     0.000000   \n",
       "75%                                     0.000000   \n",
       "max                                     1.000000   \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_D19_VERSICHERUNGEN  \\\n",
       "count                                 42833.000000   \n",
       "mean                                      0.060141   \n",
       "std                                       0.237750   \n",
       "min                                       0.000000   \n",
       "25%                                       0.000000   \n",
       "50%                                       0.000000   \n",
       "75%                                       0.000000   \n",
       "max                                       1.000000   \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_D19_VOLLSORTIMENT  \\\n",
       "count                                42833.000000   \n",
       "mean                                     0.054047   \n",
       "std                                      0.226113   \n",
       "min                                      0.000000   \n",
       "25%                                      0.000000   \n",
       "50%                                      0.000000   \n",
       "75%                                      0.000000   \n",
       "max                                      1.000000   \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_D19_WEIN_FEINKOST  \\\n",
       "count                                42833.000000   \n",
       "mean                                     0.006841   \n",
       "std                                      0.082425   \n",
       "min                                      0.000000   \n",
       "25%                                      0.000000   \n",
       "50%                                      0.000000   \n",
       "75%                                      0.000000   \n",
       "max                                      1.000000   \n",
       "\n",
       "       D19_LETZTER_KAUF_BRANCHE_nan  \n",
       "count                  42833.000000  \n",
       "mean                       0.175706  \n",
       "std                        0.380574  \n",
       "min                        0.000000  \n",
       "25%                        0.000000  \n",
       "50%                        0.000000  \n",
       "75%                        0.000000  \n",
       "max                        1.000000  \n",
       "\n",
       "[8 rows x 445 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace unknown values with Nan\n",
    "fill_unknown_values(mailout_test)\n",
    "\n",
    "# Handle string\n",
    "convert_str_to_int(mailout_test, \"CAMEO_DEUG_2015\")\n",
    "convert_str_to_int(mailout_test, \"CAMEO_INTL_2015\")\n",
    "\n",
    "convert_biclass_to_int(mailout_test, \"OST_WEST_KZ\")\n",
    "\n",
    "# Handle categorical data\n",
    "mailout_test = convert_categories_to_onehot(mailout_test, \"CAMEO_DEU_2015\")\n",
    "mailout_test = convert_categories_to_onehot(mailout_test, \"D19_LETZTER_KAUF_BRANCHE\")\n",
    "\n",
    "\n",
    "# Handle time stamps\n",
    "mailout_test['EINGEFUEGT_AM'] = pd.to_datetime(mailout_test['EINGEFUEGT_AM']).sub(pd.Timestamp('1900-01-01')).dt.days\n",
    "\n",
    "# Replace NaN with means\n",
    "for c in mailout_test.columns:\n",
    "    mailout_test[c].fillna(mailout_test[c].mean(), inplace=True)\n",
    "print([col for col in mailout_test.columns if np.sum(mailout_test[col].isna()) > 0])\n",
    "\n",
    "# Normalization with min-max\n",
    "max_mailout_test = mailout_test.max()\n",
    "min_mailout_test = mailout_test.min()\n",
    "normalized_mailout_test = (mailout_test - min_mailout_test)/(max_mailout_test - min_mailout_test)\n",
    "\n",
    "# Check the min-max of each columns. The min, max should be 0 and 1\n",
    "normalized_mailout_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42833, 445)\n",
      "42833\n"
     ]
    }
   ],
   "source": [
    "print(normalized_mailout_test.shape)\n",
    "\n",
    "# Load test data frame into XGBoost data structure\n",
    "test_matrix = xgb.DMatrix(normalized_mailout_test)\n",
    "\n",
    "# Prediction of test data\n",
    "pred = gbt.predict(test_matrix, ntree_limit=n_best_trees)\n",
    "print(len(pred))\n",
    "\n",
    "# Create data frame from test data\n",
    "df_pred_test = pd.DataFrame({\"RESPONSE\":pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'LNR', u'RESPONSE'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Save the data frame to the Kaggle-competition CSV format. (two columns of LNR and RESPONSE)\n",
    "mailout_test_save = pd.read_csv('Udacity_MAILOUT_052018_TEST.csv', sep=';')\n",
    "df = pd.DataFrame({\"LNR\":mailout_test_save[\"LNR\"], \"RESPONSE\":pred})\n",
    "print(df.columns)\n",
    "df.to_csv(\"test_pred.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
